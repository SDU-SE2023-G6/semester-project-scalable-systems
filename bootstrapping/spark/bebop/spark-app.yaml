apiVersion: spark.stackable.tech/v1alpha1
kind: SparkApplication
metadata:
  name: pyspark-btctweet
spec:
  version: "1.0"
  sparkImage: ghcr.io/sdu-se2023-g6/tweet_spark:v1.5
  mode: cluster
  mainApplicationFile: "s3a://spark-data/spark-app.py"
  logFileDirectory:
    s3:
      prefix: eventlogs/
      bucket:
        reference: spark-history
  s3connection:
    reference: data-connection
  sparkConf:
#    spark.sql.execution.arrow.pyspark.enabled: "false"  
    spark.hadoop.fs.s3a.aws.credentials.provider: "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider"
    spark.hadoop.fs.s3a.endpoint: "http://minio:9000"
    spark.jars: |
      s3a://spark-data/jars/spark-sql-kafka-0-10_2.12-3.2.0.jar,
      s3a://spark-data/jars/spark-streaming-kafka-0-10_2.12-3.2.0.jar,
      s3a://spark-data/jars/kafka-clients-3.2.0.jar,
      s3a://spark-data/jars/spark-token-provider-kafka-0-10_2.12-3.2.0.jar,
      s3a://spark-data/jars/commons-pool2-2.8.0.jar
    kafkaAdress: "strimzi-kafka-bootstrap.kafka:9092"
    checkpointSentiment: "s3a://spark-data/cp_sentiment"
    spark.kubernetes.driver.request.cores: "300m"
    spark.kubernetes.executor.request.cores: "100m"
    spark.kubernetes.driver.limit.cores: "500m"
    spark.kubernetes.executor.limit.cores: "400m"
    spark.kubernetes.driver.request.memory: "1Gi"
    spark.kubernetes.executor.request.memory: "1Gi"
    spark.kubernetes.driver.limit.memory: "1Gi"
    spark.kubernetes.executor.limit.memory: "1Gi"
    condaEnv: "/opt/pyspark_conda_env.tar.gz" 
    spark.archives: "/opt/pyspark_conda_env.tar.gz"
    spark.kubernetes.file.upload.path: "s3a://spark-data/upload"
  driver:
    resources:
      cpu:
        min: "150m"
        max: "400m"
      memory:
        limit: "1Gi"
  executor:
    instances: 3
    resources:
      cpu:
        min: "50m"
        max: "400m"
      memory:
        limit: "1Gi"
